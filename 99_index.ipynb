{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from candidate_exercises_public.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# candidate-exercises-public\n",
    "\n",
    "> This is the answer to the tech assessments for imply. Here you can find the coding exercise explained and the Case Study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `git clone https://github.com/theccalderon/imply.git`\n",
    "2. `cd imply`\n",
    "3. `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python candidate_exercises_public.py path/to/json/file path/to/avro/file path/to/csv/file path/to/final/csv/file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Coding Excerise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to combine the files, eliminating any duplicates and write to a single .CSV file sorted alphabetically by the city name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use `pandas` since is my go to when dealig with tabular data in python. Dataframes allow us to easily remove duplicates, aggregate data and create new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandavro as pdx\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(json_filepath, arvo_filepath, csv_filepath, output_filepath):\n",
    "    \"\"\"Combines the three files, eliminates duplicates and it sorts the resulting dataset by City Name. Creates \n",
    "    a csv file in output_filepath and returns a dataframe with its content\n",
    "    \"\"\"\n",
    "    #reading all the three files\n",
    "    df = pd.read_json(json_filepath)\n",
    "    df = df.append(pd.read_csv(csv_filepath))\n",
    "    df = df.append(pdx.read_avro(arvo_filepath))\n",
    "    #dropping duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    #sorting by Name\n",
    "    df = df.sort_values(by='Name')\n",
    "    #writing to csv\n",
    "    df.to_csv(output_filepath,columns=['Name','CountryCode','Population'], index=False)\n",
    "    return pd.read_csv(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv = combine_files('CityListA.json', 'CityListB.avro', 'CityList.csv', 'finalDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_core.ipynb              arvo.csv\r\n",
      "99_index.ipynb             \u001b[1m\u001b[36mcandidate_exercises_public\u001b[m\u001b[m\r\n",
      "CONTRIBUTING.md            csv.csv\r\n",
      "CityList.csv               \u001b[1m\u001b[36mdocs\u001b[m\u001b[m\r\n",
      "CityListA.json             finalDataset.csv\r\n",
      "CityListB.avro             json.csv\r\n",
      "LICENSE                    settings.ini\r\n",
      "Makefile                   setup.py\r\n",
      "README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`finalDataset.csv` is the resulting csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A CoruÃ±a (La CoruÃ±a)</td>\n",
       "      <td>ESP</td>\n",
       "      <td>243402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aalborg</td>\n",
       "      <td>DNK</td>\n",
       "      <td>161161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abadan</td>\n",
       "      <td>IRN</td>\n",
       "      <td>206073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>CAN</td>\n",
       "      <td>105403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>GBR</td>\n",
       "      <td>213070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name CountryCode  Population\n",
       "0  A CoruÃ±a (La CoruÃ±a)         ESP      243402\n",
       "1                 Aalborg         DNK      161161\n",
       "2                  Abadan         IRN      206073\n",
       "3              Abbotsford         CAN      105403\n",
       "4                Aberdeen         GBR      213070"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the count of all rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the count of rows we can just check read the final csv file (created by `combine_files`) and check its length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the city with the largest population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the city with the largest population we can sort by Population descendingly and grab the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_with_largets_population_from_df(df):\n",
    "    return df.sort_values(by='Population',ascending=False)[:1][['Name']].iloc[0]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumbai (Bombay)\n"
     ]
    }
   ],
   "source": [
    "print(city_with_largets_population_from_df(final_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's not a suprise isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the total population of all cities in Brazil (CountryCode == BRA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the total population of all cities in Brazil we select the rows where CountryCode == BRA and then sum the population values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_population_per_country_from_df(df, country_code):\n",
    "    return df.loc[df['CountryCode']==country_code]['Population'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55955012\n"
     ]
    }
   ],
   "source": [
    "print(total_population_per_country_from_df(final_csv,'BRA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What changes could be made to improve your program's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is currently being read sequentially (one file at the time). To improve performance, we could read the data in parallel since it's coming form different sources. By doing this we would maximize the use of our resources and speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you scale your solution to a much larger dataset (too large for a single machine to store)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale the solution when the data cannot be stored just in a single machine we can use big data technologies (like Hadoop) which allow clustering multiple computers to analyze massive datasets in parallel more quickly. Hadoop makes it easier to use all the storage and processing capacity in cluster servers, and to execute distributed processes against huge amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset structure:\n",
    "    \n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"2016-08-04T18:05:07.054Z\",\n",
    "  \"session\": \"S74650219\",\n",
    "  \"remote_address\": \"172.31.3.170\",\n",
    "  \"path\": \"http://www.koalastothemax.com/img/koalas3.jpg\",\n",
    "  \"referrer\": \"Direct\",\n",
    "  \"timezone_offset\": \"-120\",\n",
    "  \"language\": \"it-IT\",\n",
    "  \"city\": \"Borgo San Lorenzo\",\n",
    "  \"region\": \"Province of Florence\",\n",
    "  \"country\": \"Italy\",\n",
    "  \"continent\": \"Europe\",\n",
    "  \"latitude\": 43.9555,\n",
    "  \"longitude\": 11.3856,\n",
    "  \"browser\": \"Mozilla\",\n",
    "  \"browser_version\": \"rv:11.0\",\n",
    "  \"agent_type\": \"Browser\",\n",
    "  \"agent_category\": \"Personal computer\",\n",
    "  \"os\": \"Windows\",\n",
    "  \"platform\": \"Windows\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The site gets 200 million hits per day which means approx:\n",
    "* 73 billion hits a year\n",
    "* 6 billion hits a month\n",
    "* 8,333,333 hits per hour\n",
    "* 138,888 hits per minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They need to answer questions mainly about unique sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The on prem hardware specs is:\n",
    "    - 2x 8-core HT processors (16 cores total, 32 hardware threads) 64GB memory 600GB SSD disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many servers will be necessary for an analytics cluster for one year of this dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, I'm assuming the Koala already has a deep storage system (S3, HDFS, local....) set up to store the segments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the traffic is very high we are going to use a cluster deployment. One of the main advantage of using clustering is the ability to scale horizontally on demand. That allows us to start small and then increase our hardware if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Production setup, here is my recommendation (we need a highly available and fault tolerant setup, see https://docs.imply.io/on-prem/deploy/cluster#production-setup):\n",
    "* 3 Master servers\n",
    "* 2 query servers\n",
    "* 4 data servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with that and monitor (using Clarity) in case we need to scale out any resources. My main concern when analyzing this question was the **data servers**, but since those can be added on the fly, we can start with 4 and increase if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reasearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- roll up and metrics at ingestion time\n",
    "- break apart the Data server components (see https://medium.com/airbnb-engineering/druid-airbnb-data-platform-601c312f2a4c as example) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* https://docs.imply.io/on-prem/deploy/cluster\n",
    "* https://druid.apache.org/docs/latest/tutorials/cluster.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How these servers should be configured (JVM config, Druid runtime.properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisson between r4.2xlarge and Koala's hardware (https://aws.amazon.com/ec2/instance-types/ and https://en.wikichip.org/wiki/intel/xeon_e5/e5-2686_v4):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             | Koala's server | AWS r4.2xlarge |\n",
    "|-------------|----------------|----------------|\n",
    "| Cores total | 16             | 18             |\n",
    "| Threads     | 32             | 36             |\n",
    "| RAM         | 64             | 61             |\n",
    "| Disk        | 600 GB SSD     | Configurable   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per (https://docs.imply.io/on-prem/deploy/cluster#configuration-tuning)\n",
    ">If you are using r4.2xlarge EC2 instances or similar hardware, the configuration in the distribution is a reasonable starting point.\n",
    "\n",
    "Based on the table above, we can see the hardware it's actually similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jvm.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the example `jvm.config` for our servers since the amount of RAM in the examples (https://docs.imply.io/on-prem/deploy/cluster) is equal to Koala's RAM per server. (see `jvm.config` files in `imply-3.1.6/dist/druid/conf/druid/cluster/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runtime.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to list some of the values below, for more information see the `runtime.properties` files in `imply-3.1.6/dist/druid/conf/druid/cluster`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data/middleManager/runtime.properties:druid.server.http.numThreads=60\\\n",
    "data/historical/runtime.properties:druid.server.http.numThreads=60\\\n",
    "query/broker/runtime.properties:druid.server.http.numThreads=60\\\n",
    "query/router/runtime.properties:druid.server.http.numThreads=100\n",
    "\n",
    "data/historical/runtime.properties:druid.cache.sizeInBytes=256000000\\\n",
    "data/middleManager/runtime.properties:druid.indexer.fork.property.druid.processing.buffer.sizeBytes=100000000\\\n",
    "data/historical/runtime.properties:druid.processing.buffer.sizeBytes=500000000\\\n",
    "query/broker/runtime.properties:druid.processing.buffer.sizeBytes=500000000\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Imply can be used to answer the two sample queries provided by Koala (number of unique sessions in a particular month; number of unique sessions per country in a particular day)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Pivot` or `SQL` to answer the questions above. Please see below how to answer the questions using `SQL`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of unique sessions in a particular month\n",
    "```sql\n",
    "    SELECT COUNT(DISTINCT(session)) AS unique_sessions\n",
    "    FROM koala_sessions\n",
    "    WHERE \"__time\" BETWEEN TIMESTAMP 'YYYY-MM-01 00:00:00' AND TIMESTAMP 'YYYY-(MM+1)-01 00:00:00'\n",
    "```\n",
    "    where koala_sessions is the datasource name and MM will be the month we want to get the information for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of unique sessions per country in a particular day\n",
    "```sql\n",
    "    SELECT COUNT(DISTINCT (session)) AS unique_sessions, country\n",
    "    FROM koala_sessions\n",
    "    WHERE \"__time\" BETWEEN TIMESTAMP 'YYYY-MM-DD 00:00:00' AND TIMESTAMP 'YYYY-MM-(DD+1) 00:00:00'\n",
    "    GROUP BY country\n",
    "```\n",
    "    where koala_sessions is the datasource name and YYYY == year, MM == month, DD == day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:candidate-exercises-public]",
   "language": "python",
   "name": "conda-env-candidate-exercises-public-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
